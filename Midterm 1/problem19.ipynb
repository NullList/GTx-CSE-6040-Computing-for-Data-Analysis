{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 3: Gaussian Naïve Bayes Classification for Predicting Protein Localization Sites\n",
    "\n",
    "_Version 1.2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This notebook concerns a machine learning method called the _(Gaussian) naïve Bayes classifier_. It finds uses in text categorization, spam filters, and biomedicine.\n",
    "\n",
    "In this problem, you'll apply naïve Bayes to the problem of predicting protein localization sites in [E.Coli bacteria](https://en.wikipedia.org/wiki/Escherichia_coli). In essence, the problem is to estimate where particular proteins reside in a cell. If, later on, you want to learn more, see the references at the end of this notebook.\n",
    "\n",
    "This notebook asks you to implement the method using only constructs from basic Python, without auxiliary libraries like `numpy` or `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Setup and data\n",
    "\n",
    "You will use a modified version of [a publicly available dataset](https://archive.ics.uci.edu/ml/datasets/ecoli). Run the code cell below to load this data, whose output we'll explain afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ecoli-mod.mat...\n",
      "\n",
      "Some information about the given data:\n",
      "218 total training samples having 5 features each\n",
      "109 total testing samples having 5 features each\n",
      "\n",
      "Training data for first 5 samples:\n",
      "[{'feature_1': 0.44, 'feature_2': 0.28, 'feature_3': 0.43, 'feature_4': 0.27, 'feature_5': 0.37},\n",
      " {'feature_1': 0.31, 'feature_2': 0.36, 'feature_3': 0.58, 'feature_4': 0.94, 'feature_5': 0.94},\n",
      " {'feature_1': 0.58, 'feature_2': 0.55, 'feature_3': 0.57, 'feature_4': 0.7, 'feature_5': 0.74},\n",
      " {'feature_1': 0.38, 'feature_2': 0.44, 'feature_3': 0.43, 'feature_4': 0.2, 'feature_5': 0.31},\n",
      " {'feature_1': 0.29, 'feature_2': 0.28, 'feature_3': 0.5, 'feature_4': 0.42, 'feature_5': 0.5}]\n",
      "\n",
      "Training results for first 5 samples:\n",
      "['class_1', 'class_2', 'class_2', 'class_1', 'class_1']\n",
      "\n",
      "The possible class labels are: {'class_5', 'class_3', 'class_1', 'class_4', 'class_2'}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from problem_utils import load_data\n",
    "from pprint import pprint # For pretty-printing Python data structures\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data('ecoli-mod.mat')\n",
    "\n",
    "print (\"\\nTraining data for first 5 samples:\")\n",
    "pprint(x_train[:5], width=100)\n",
    "print (\"\\nTraining results for first 5 samples:\")\n",
    "print(y_train[:5])\n",
    "print(f\"\\nThe possible class labels are: {set(y_train + y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**About these data.** The data is split into _training data_, which you'll use to build a predictive model, and _testing data_, which you'll use to test the accuracy of the model. There are four variables of interest: `x_train` and `y_train`, which hold the training data, and `x_test` and `y_test`, which hold the testing data. More specifically:\n",
    "\n",
    "- `x_train` is a _list of dictionaries_. Each element `x_train[i]` is the `i`-th data point of the training set. The point is represented by a _feature vector_, which is a linear algebraic vector having five components. Each vector is stored as a dictionary, with its components named by the keys `'feature_1'` through `'feature_5'`.\n",
    "\n",
    "- `y_train` is a _list of strings_. Each element `y_train[i]` is a _class label_ for the `i`-th data point. Observe from the output above that there are five possible class labels, `'class_1'` through `'class_5'`.\n",
    "\n",
    "- `x_test` and `y_test` are similar to the above, except that they hold values for the test data. Our goal is to build a model of the training data that can closely predict the true class labels, `y_test`, given only the feature vectors in `x_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Background on Bayes' Theorem\n",
    "\n",
    "Recall _Bayes' theorem_ (or Bayes' law or Bayes' rule) from Topic 3. It is a statement about the quantitative relationships among the [conditional probablilities](https://stats.stackexchange.com/questions/239014/bayes-theorem-intuition) (recall Notebook 2) of several events. One common use of Bayes' theorem is to \"reverse\" a conditional relationship, such as estimating the probability that [rich people are happy](https://www.quora.com/What-is-an-intuitive-explanation-of-Bayes-Rule) given knowledge of the probability that a happy person is rich. Another use is to [update one's belief](https://arbital.com/p/bayes_rule/?l=1zq) using prior knowledge when new information arrives, like updating the probability that a person has cancer when he or she now tests positive, knowing some background information on the accuracy of the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The mathematical statement of Bayes' theorem is\n",
    "\n",
    "$${\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)P(A)}{P(B)}}},$$\n",
    "\n",
    "where ${\\displaystyle A}$ and ${\\displaystyle B}$ are events and ${\\displaystyle P(B)\\neq 0}$.\n",
    "\n",
    "- ${\\displaystyle P(A\\mid B)}$ is a posterior probability of event ${\\displaystyle A}$ given event ${\\displaystyle B}$, or just **posterior**.\n",
    "- ${\\displaystyle P(B\\mid A)}$ is a conditional probability of event ${\\displaystyle B}$ given event ${\\displaystyle A}$, also called **likelihood**.\n",
    "- ${\\displaystyle P(A)}$ is prior probability of event ${\\displaystyle A}$ independently of ${\\displaystyle B}$, also called just **prior**.\n",
    "- ${\\displaystyle P(B)}$ is the probabilities of observing ${\\displaystyle B}$ independently ${\\displaystyle A}$, also called marginal likelihood or model **evidence**.\n",
    "\n",
    "In words, the formula would be\n",
    "\n",
    "$$ \\text{posterior} \\ = \\ \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{evidence}}.$$\n",
    "\n",
    "You can read more about Bayes' Theorem on its [Wiki page](https://en.wikipedia.org/wiki/Bayes%27_theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Part 0: The Naïve Bayes Model\n",
    "\n",
    "We can use Bayes' theorem to make predictions not only for toy problems with two events, but also for much more complex problems, such as multinomial classification for data with multiple features. However, finding the likelihood of a multidimensional feature vector given the assigned class can be very hard and even [intractable](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Probabilistic_model). However, we can greatly simplify the computation if we make a _naïve_ assumption that all the features are conditionally independent. In this case, the probability of observing a class label for a given feature vector becomes\n",
    "\n",
    "$${\\displaystyle p(C_{k}\\mid \\mathbf {x} )={\\frac {p(C_{k})\\ \\prod _{i=1}^{n}p(x_{i}\\mid C_{k})}{p(\\mathbf {x} )}}\\,}$$\n",
    "\n",
    "Moreover, since evidence $p(\\mathbf {x} )$ does not depend on $C$, in practice we can omit it and use the formula\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}p(C_{k}\\mid \\mathbf {x} )&\\varpropto p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})\\,\\end{aligned}}} \\tag{1}$$\n",
    "\n",
    "where ${\\displaystyle \\varpropto }$ denotes proportionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Learning class priors.** Before moving on, let's write a function that computes class priors, $p(C_k)$ using the vector of class assignments. Recall that `y_train` holds the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 0** (2 points). Write a function `prior(data)` that takes a list of class values as inputs and returns a dictionary containing the priors. In this dictionary, the class names serve as keys and the probability of the occurrence of each class as values.\n",
    "\n",
    "We can find prior of class $K$ simply dividing number of data samples of class $K$ by total number of data samples. For example, if\n",
    "\n",
    "```python\n",
    "data = ['class_1' , 'class_2' , 'class_3' , 'class_4' , 'class_3' , 'class_2' , 'class_1' , 'class_3' ]\n",
    "```\n",
    "\n",
    "then your function should produce the output,\n",
    "\n",
    "```python\n",
    "prior(data) == {'class_1': 0.25,'class_2': 0.25,'class_3': 0.375, 'class_4': 0.125 }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prior(data):\n",
    "    assert isinstance(data, list), f\"Input `data` has type `{type(data)}`, which does not derive from `list` as expected.\"\n",
    "    from collections import Counter\n",
    "    count = Counter (data)\n",
    "    return {k:v/sum(count.values()) for k,v in count.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_prior_script",
     "locked": true,
     "points": "2",
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test cell: `test_prior_script`\n",
    "\n",
    "#Test Case 1\n",
    "test_data = ['a1' , 'a2' , 'b1' , 'b2' , 'b1' , 'a2' , 'a1' , 'b1' ]\n",
    "p1 = prior(test_data)\n",
    "p1_test = {'a1': 0.25,'a2': 0.25,'b1': 0.375, 'b2': 0.125 }\n",
    "assert isinstance(p1, dict), f\"Input `p1` has type `{type(p1)}`, which does not derive from `dict` as expected.\"\n",
    "assert p1 == p1_test, f\"The Output for Test Case 1 is incorrect, returned: \\n{p1}\\n, should be: \\n{p1_test}\\n\"\n",
    "\n",
    "#Test Case 2\n",
    "test_data2 = ['a1' , 'a2' , 'b1' , 'b2' , 'b1' , 'a2' , 'a1' , 'b1' , 'a1' , 'a2' , 'b1' , 'b2' , 'b1' , 'a2' , 'a1' , 'b1' , 'a2' , 'a2' , 'b1' , 'b2' , 'b1' , 'c1' , 'a1' , 'b2' , 'a1' , 'b2' , 'b1' , 'c2' , 'b1' , 'a2' , 'a3' , 'b1',  'a3' , 'a2' , 'b4' , 'b2' , 'b1' , 'a2' , 'a1' , 'b1' , 'a3' , 'a2' , 'b1' , 'b2' , 'b1' , 'a2' , 'a3' , 'b2' ]\n",
    "p2 = prior(test_data2)\n",
    "p2_test = {'a1': 0.14583333, 'a2': 0.22916667, 'b1': 0.3125, 'b2': 0.16666667, 'c1': 0.02083333, 'c2': 0.02083333, 'a3': 0.08333333, 'b4': 0.02083333}\n",
    "assert isinstance(p2, dict), f\"Input `p2` has type `{type(p2)}`, which does not derive from `dict` as expected.\"\n",
    "for k in p2_test.keys():\n",
    "    assert math.isclose(p2[k], p2_test[k], abs_tol=1e-8), f\"The Output for Test Case 2 is incorrect, returned: \\n{p2:.8f}\\n, should be: \\n{p2_test}\\n\"\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Assuming your implementation really is correct, run the code below to inspect the priors for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Priors:\n",
      "{'class_1': 0.4724770642201835, 'class_2': 0.23394495412844038, 'class_4': 0.10091743119266056, 'class_3': 0.14220183486238533, 'class_5': 0.05045871559633028}\n"
     ]
    }
   ],
   "source": [
    "prior_val = prior(y_train)\n",
    "print(\"\\nClass Priors:\")\n",
    "print(prior_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Part 1: The Gaussian Naïve Bayes model\n",
    "\n",
    "In real problems we often do not know the true underlying data distributions. Instead, we estimate them from the data, often assuming the form (but not the parameters) of the data distribution.\n",
    "\n",
    "When dealing with continuous data, a typical assumption is that any continuous values associated with each class are distributed according to a [normal (or Gaussian) distribution](https://en.wikipedia.org/wiki/Normal_distribution). Under this assumption, the likelihood is\n",
    "\n",
    "$${\\displaystyle p(x=f_i\\mid C_{k})= N(f_i;µ_{i,k},σ_{i,k}^2)}$$\n",
    "\n",
    "$${\\displaystyle p(x=f_i\\mid C_{k})={\\frac {1}{\\sqrt {2\\pi \\sigma _{i,k}^{2}}}}\\, \\exp\\left({-{\\frac {(f_i-\\mu _{i,k})^{2}}{2\\sigma _{i,k}^{2}}}}\\right)} \\tag{2}$$\n",
    "\n",
    "where $µ_{i,k}$ is the mean of feature $f_i$ and $σ_{i,k}^2$ is its variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1.a** (1 point). Write two functions, `mean(vector)` and `var(vector)`, to compute the mean and variance of the components of an input vector, respectively. The vector, `vector`, is given as a list of values and you need to return the mean and variance of those values.\n",
    "\n",
    "Recall that the mean and variance are defined by\n",
    "\n",
    "$$ \\mathbf {mean}(\\mathbf {x}) = \\mu(\\mathbf {x})= \\frac{1}{n}\\sum_{i=1}^n{x_i} \\quad \\mbox{and} \\quad\n",
    "\\mathbf {var}(\\mathbf {x}) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
    "$$\n",
    "\n",
    "For example, suppose `vector = [1, 2, 3]`. Then,\n",
    "\n",
    "```python\n",
    "mean([1, 2, 3]) = 2.0 \n",
    "var([1, 2, 3]) = 0.66666667\n",
    "```\n",
    "\n",
    "In the context of our problem, these functions would be helpful while implementing equation (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(vector):\n",
    "    return sum(vector)/len(vector)\n",
    "\n",
    "def var(vector):\n",
    "    avg = mean(vector)\n",
    "    return sum([(x - avg)**2 for x in vector]) / len(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_mean_var",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test cell: `test_mean_var`\n",
    "\n",
    "#Test Case 1\n",
    "l = [1.0 , 5.272 , 6.2734 , 32.4824 , 8.2876 , 43.3242 ]\n",
    "m = mean(l)\n",
    "v = var(l)\n",
    "assert isinstance(m, float), f\"Input `m` has type `{type(m)}`, which does not derive from `float` as expected.\"\n",
    "assert isinstance(v, float), f\"Input `v` has type `{type(v)}`, which does not derive from `float` as expected.\"\n",
    "assert math.isclose(m, 16.1066, abs_tol=1e-8), f\"Test Case 1 for Mean Failed as `{m:.8f}` != 16.1066\"\n",
    "assert math.isclose(v, 252.06517989, abs_tol=1e-8), f\"Test Case 1 for Variance Failed as `{v:.8f}` != 252.06517989\"\n",
    "\n",
    "#Test Case 2\n",
    "l1 = [11.0423 , 6.34324 , 7.347234 , 426.244 , 247. , 232.4332476 , 9. , 9. ,9. , -3.432 , 0. , -34.234 ]\n",
    "m1 = mean(l1)\n",
    "v1 = var(l1)\n",
    "assert isinstance(m1, float), f\"Input `m` has type `{type(m1)}`, which does not derive from `float` as expected.\"\n",
    "assert isinstance(v1, float), f\"Input `v` has type `{type(v1)}`, which does not derive from `float` as expected.\"\n",
    "assert math.isclose(m1, 76.64533513, abs_tol=1e-8), f\"Test Case 2 for Mean Failed as `{m1:.8f}` != 76.64533513\"\n",
    "assert math.isclose(v1, 18988.91413866, abs_tol=1e-8), f\"Test Case 2 for Variance Failed as `{v1:.8f}` != 18988.91413866\"\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1.b** (2 points). Using `mean(l)` and `var(l)`, implement a function, `likelihood(x, y)`, to compute the parameters of the Gaussian likelihood of the data, `x` and `y`.\n",
    "\n",
    "In particular, the input `x` is a list of dictionaries containing feature vectors (e.g., `x_train` from the training data) and `y` is a list of class labels (e.g., `y_train`). The output is a _pair of dictionaries of dictionaries_ (yikes!!), one holding means and the other variances. It's easiest to understand the inputs and outputs by example, so let's start there.\n",
    "\n",
    "Suppose the inputs `x` and `y` correspond to four data points, where the feature vectors have three components and there are two distinct class labels:\n",
    "\n",
    "```python\n",
    "x = [{'feature_1': 0.58, 'feature_2': 0.55, 'feature_3': 0.57},\n",
    "     {'feature_1': 0.38, 'feature_2': 0.44, 'feature_3': 0.43},\n",
    "     {'feature_1': 0.29, 'feature_2': 0.28, 'feature_3': 0.5},\n",
    "     {'feature_1': 0.98, 'feature_2': 0.74, 'feature_3': 0.32}] \n",
    "y = ['class_1', 'class_2', 'class_1', 'class_2']\n",
    "```\n",
    "\n",
    "Your function should return two outputs,\n",
    "\n",
    "```python\n",
    "#Note: The result is spread out in different lines to provide more clarity\n",
    "dist_mean, dist_var = likelihood(x_train, y_train)\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "```python\n",
    "# Dictionary corresponding to Mean of each feature in a particular class. \n",
    "dist_mean == {'class_1': {'feature_1': 0.435, 'feature_2': 0.415, 'feature_3': 0.535},\n",
    "              'class_2': {'feature_1': 0.680, 'feature_2': 0.59, 'feature_3': 0.375}}\n",
    "\n",
    "# Dictionary corresponding to Variance of each feature in a particular class.\n",
    "dist_var == {'class_2': {'feature_1': 0.09, 'feature_2': 0.0225, 'feature_3': 0.003025},\n",
    "             'class_1': {'feature_1': 0.021025, 'feature_2': 0.018225, 'feature_3': 0.001225}}\n",
    "```\n",
    "\n",
    "Consider `dist_mean`. It is a dictionary whose keys are class labels and whose values are _mean_ feature vectors. For instance, consider the vectors for the `'class_1'` data points in `x`. In the `'feature_1'` component, the values that occur are 0.58 and 0.29; therefore, `dist_mean['class_1']['feature_1'] == (0.58 + 0.29) / 2 == 0.435`.\n",
    "\n",
    "In the function you are to complete, below, we've created two empty dictionaries to hold your results and return them. You need to supply the code that populates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood(x, y):\n",
    "    dist_mean = {}\n",
    "    dist_var = {}\n",
    "    for label in set(y):\n",
    "        count_list = [x[n] for n in range(len(x)) if y[n] == label]\n",
    "        dist_mean[label] = {k: mean([el[k] for el in count_list]) for k in count_list[0].keys()}\n",
    "        dist_var[label] = {k: var([el[k] for el in count_list]) for k in count_list[0].keys()}\n",
    "    return dist_mean, dist_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_likelihood",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test cell: `test_likelihood`\n",
    "\n",
    "#Test Case 1\n",
    "training_data = [{'feature_1': 0.58, 'feature_2': 0.55, 'feature_3': 0.57, 'feature_4': 0.7, 'feature_5': 0.74},{'feature_1': 0.38, 'feature_2': 0.44, 'feature_3': 0.43, 'feature_4': 0.2, 'feature_5': 0.31},{'feature_1': 0.29, 'feature_2': 0.28, 'feature_3': 0.5, 'feature_4': 0.42, 'feature_5': 0.5},{'feature_1': 0.98, 'feature_2': 0.74, 'feature_3': 0.32, 'feature_4': 0.25, 'feature_5': 0.11},{'feature_1': 0.08, 'feature_2': 0.69, 'feature_3': 0.84, 'feature_4': 0.85, 'feature_5': 0.17} ] \n",
    "\n",
    "training_result = ['class_1' , 'class_2' , 'class_1' , 'class_2','class_2']\n",
    "\n",
    "mean1, var1 = likelihood(training_data, training_result)\n",
    "assert isinstance(mean1, dict), f\"Input `mean1` has type `{type(mean1)}`, which does not derive from `dict` as expected.\"\n",
    "assert isinstance(var1, dict), f\"Input `var1` has type `{type(var1)}`, which does not derive from `dict` as expected.\"\n",
    "mean1_test = {'class_1': {'feature_1': 0.435, 'feature_2': 0.415, 'feature_3': 0.535, 'feature_4': 0.56, 'feature_5': 0.62},'class_2': {'feature_1': 0.48, 'feature_2': 0.62333333, 'feature_3': 0.53, 'feature_4': 0.43333333, 'feature_5': 0.19666667}}\n",
    "var1_test = {'class_1': {'feature_1': 0.021025, 'feature_2': 0.018225, 'feature_3': 0.001225, 'feature_4': 0.0196, 'feature_5': 0.0144},'class_2': {'feature_1': 0.14, 'feature_2': 0.01722222, 'feature_3': 0.05006667, 'feature_4': 0.08722222, 'feature_5': 0.00702222}}\n",
    "\n",
    "for cl in mean1_test.keys():\n",
    "    for f in mean1_test[cl].keys():\n",
    "        assert math.isclose(mean1[cl][f], mean1_test[cl][f], abs_tol=1e-8), f\"The Output for Test Case 1 is incorrect, returned: \\n{mean1:.8f}\\n, should be: \\n{mean1_test}\\n\"\n",
    "for cl in var1_test.keys():\n",
    "    for f in var1_test[cl].keys():\n",
    "        assert math.isclose(var1[cl][f], var1_test[cl][f], abs_tol=1e-8), f\"The Output for Test Case 1 is incorrect, returned: \\n{var1:.8f}\\n, should be: \\n{var1_test}\\n\"\n",
    "\n",
    "#Test Case 2\n",
    "training_data2 = [{'feature_1': 0.9238, 'feature_2': 0.34, 'feature_3': 0.57, 'feature_4': 0.7, 'feature_5': 0.747},{'feature_1': 0.3842, 'feature_2': 0.4234, 'feature_3': 0.2343, 'feature_4': 0.2, 'feature_5': 0.331},{'feature_1': 0.2129, 'feature_2': 0.0228, 'feature_3': 0.425, 'feature_4': 0.835, 'feature_5': 0.587}] \n",
    "\n",
    "training_result2 = ['class_1' , 'class_2' , 'class_1']\n",
    "\n",
    "mean2, var2 = likelihood(training_data2, training_result2)\n",
    "assert isinstance(mean2, dict), f\"Input `mean2` has type `{type(mean2)}`, which does not derive from `dict` as expected.\"\n",
    "assert isinstance(var2, dict), f\"Input `var2` has type `{type(var2)}`, which does not derive from `dict` as expected.\"\n",
    "mean2_test = {'class_1': {'feature_1': 0.56835, 'feature_2': 0.1814, 'feature_3': 0.4975, 'feature_4': 0.7675, 'feature_5': 0.667},'class_2': {'feature_1': 0.3842, 'feature_2': 0.4234, 'feature_3': 0.2343, 'feature_4': 0.2, 'feature_5': 0.331} }\n",
    "var2_test = {'class_1': {'feature_1': 0.1263447, 'feature_2': 0.02515396, 'feature_3': 0.00525625, 'feature_4': 0.00455625, 'feature_5': 0.0064},'class_2': {'feature_1': 0.0, 'feature_2': 0.0, 'feature_3': 0.0, 'feature_4': 0.0, 'feature_5': 0.0}}\n",
    "\n",
    "for cl in mean2_test.keys():\n",
    "    for f in mean2_test[cl].keys():\n",
    "        assert math.isclose(mean2[cl][f], mean2_test[cl][f], abs_tol=1e-8), f\"The Output for Test Case 2 is incorrect, returned: \\n{mean2:.8f}\\n, should be: \\n{mean2_test}\\n\"\n",
    "for cl in var2_test.keys():\n",
    "    for f in var2_test[cl].keys():\n",
    "        assert math.isclose(var2[cl][f], var2_test[cl][f], abs_tol=1e-8), f\"The Output for Test Case 2 is incorrect, returned: \\n{var2:.8f}\\n, should be: \\n{var2_test}\\n\"\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Inspecting the results.** Now that you have successfully defined the function to calculate the likelihood, let us see the output of the function. We have also provided a helper function for pretty printing the mean and variance dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for pretty printing the Mean and Variance dictionaries.\n",
    "def pretty_print_mean_var(m,v):\n",
    "    \n",
    "    # Convert the contents of a dictionary to be used for displaying in a user friendly manner.\n",
    "    def formatted_dict(d):\n",
    "        import json\n",
    "        return json.dumps(d,sort_keys=True,indent=4)\n",
    "        \n",
    "    print(\"\\nPretty Printing Output:\")\n",
    "    print(\"Mean:\\n\",formatted_dict(m))\n",
    "    print(\"Variance:\\n\",formatted_dict(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Output:\n",
      "\n",
      "Mean:\n",
      " {'class_5': {'feature_1': 0.6890909090909091, 'feature_2': 0.69, 'feature_3': 0.7672727272727272, 'feature_4': 0.48181818181818187, 'feature_5': 0.3154545454545455}, 'class_3': {'feature_1': 0.651290322580645, 'feature_2': 0.7158064516129031, 'feature_3': 0.4303225806451614, 'feature_4': 0.47000000000000014, 'feature_5': 0.3870967741935484}, 'class_1': {'feature_1': 0.3594174757281553, 'feature_2': 0.4072815533980582, 'feature_3': 0.45339805825242735, 'feature_4': 0.3079611650485437, 'feature_5': 0.3930097087378642}, 'class_4': {'feature_1': 0.7390909090909091, 'feature_2': 0.47045454545454546, 'feature_3': 0.5813636363636364, 'feature_4': 0.7490909090909091, 'feature_5': 0.7690909090909092}, 'class_2': {'feature_1': 0.47176470588235286, 'feature_2': 0.5054901960784315, 'feature_3': 0.526862745098039, 'feature_4': 0.7554901960784315, 'feature_5': 0.7101960784313728}}\n",
      "Variance:\n",
      " {'class_5': {'feature_1': 0.004190082644628099, 'feature_2': 0.010818181818181817, 'feature_3': 0.004256198347107437, 'feature_4': 0.006651239669421486, 'feature_5': 0.0107702479338843}, 'class_3': {'feature_1': 0.010817689906347553, 'feature_2': 0.021353381893860563, 'feature_3': 0.006370863683662852, 'feature_4': 0.012438709677419358, 'feature_5': 0.01572382934443288}, 'class_1': {'feature_1': 0.015135582995569788, 'feature_2': 0.007635328494674332, 'feature_3': 0.009022433782637385, 'feature_4': 0.009072542181166932, 'feature_5': 0.008968611556225848}, 'class_4': {'feature_1': 0.010844628099173555, 'feature_2': 0.009504338842975206, 'feature_3': 0.003939049586776858, 'feature_4': 0.005635537190082647, 'feature_5': 0.004780991735537189}, 'class_2': {'feature_1': 0.04263414071510957, 'feature_2': 0.007828681276432143, 'feature_3': 0.016327412533640905, 'feature_4': 0.012295347943098807, 'feature_5': 0.036327412533640906}}\n",
      "\n",
      "Pretty Printing Output:\n",
      "Mean:\n",
      " {\n",
      "    \"class_1\": {\n",
      "        \"feature_1\": 0.3594174757281553,\n",
      "        \"feature_2\": 0.4072815533980582,\n",
      "        \"feature_3\": 0.45339805825242735,\n",
      "        \"feature_4\": 0.3079611650485437,\n",
      "        \"feature_5\": 0.3930097087378642\n",
      "    },\n",
      "    \"class_2\": {\n",
      "        \"feature_1\": 0.47176470588235286,\n",
      "        \"feature_2\": 0.5054901960784315,\n",
      "        \"feature_3\": 0.526862745098039,\n",
      "        \"feature_4\": 0.7554901960784315,\n",
      "        \"feature_5\": 0.7101960784313728\n",
      "    },\n",
      "    \"class_3\": {\n",
      "        \"feature_1\": 0.651290322580645,\n",
      "        \"feature_2\": 0.7158064516129031,\n",
      "        \"feature_3\": 0.4303225806451614,\n",
      "        \"feature_4\": 0.47000000000000014,\n",
      "        \"feature_5\": 0.3870967741935484\n",
      "    },\n",
      "    \"class_4\": {\n",
      "        \"feature_1\": 0.7390909090909091,\n",
      "        \"feature_2\": 0.47045454545454546,\n",
      "        \"feature_3\": 0.5813636363636364,\n",
      "        \"feature_4\": 0.7490909090909091,\n",
      "        \"feature_5\": 0.7690909090909092\n",
      "    },\n",
      "    \"class_5\": {\n",
      "        \"feature_1\": 0.6890909090909091,\n",
      "        \"feature_2\": 0.69,\n",
      "        \"feature_3\": 0.7672727272727272,\n",
      "        \"feature_4\": 0.48181818181818187,\n",
      "        \"feature_5\": 0.3154545454545455\n",
      "    }\n",
      "}\n",
      "Variance:\n",
      " {\n",
      "    \"class_1\": {\n",
      "        \"feature_1\": 0.015135582995569788,\n",
      "        \"feature_2\": 0.007635328494674332,\n",
      "        \"feature_3\": 0.009022433782637385,\n",
      "        \"feature_4\": 0.009072542181166932,\n",
      "        \"feature_5\": 0.008968611556225848\n",
      "    },\n",
      "    \"class_2\": {\n",
      "        \"feature_1\": 0.04263414071510957,\n",
      "        \"feature_2\": 0.007828681276432143,\n",
      "        \"feature_3\": 0.016327412533640905,\n",
      "        \"feature_4\": 0.012295347943098807,\n",
      "        \"feature_5\": 0.036327412533640906\n",
      "    },\n",
      "    \"class_3\": {\n",
      "        \"feature_1\": 0.010817689906347553,\n",
      "        \"feature_2\": 0.021353381893860563,\n",
      "        \"feature_3\": 0.006370863683662852,\n",
      "        \"feature_4\": 0.012438709677419358,\n",
      "        \"feature_5\": 0.01572382934443288\n",
      "    },\n",
      "    \"class_4\": {\n",
      "        \"feature_1\": 0.010844628099173555,\n",
      "        \"feature_2\": 0.009504338842975206,\n",
      "        \"feature_3\": 0.003939049586776858,\n",
      "        \"feature_4\": 0.005635537190082647,\n",
      "        \"feature_5\": 0.004780991735537189\n",
      "    },\n",
      "    \"class_5\": {\n",
      "        \"feature_1\": 0.004190082644628099,\n",
      "        \"feature_2\": 0.010818181818181817,\n",
      "        \"feature_3\": 0.004256198347107437,\n",
      "        \"feature_4\": 0.006651239669421486,\n",
      "        \"feature_5\": 0.0107702479338843\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dist_mean, dist_var = likelihood(x_train,y_train)\n",
    "\n",
    "print(\"\\nOriginal Output:\\n\")\n",
    "print(\"Mean:\\n\",dist_mean)\n",
    "print(\"Variance:\\n\",dist_var)\n",
    "\n",
    "pretty_print_mean_var(dist_mean,dist_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Great Work! Now that we have computed the conditional mean and variance, we can finally move onto the core part of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Gaussian Naïve Bayes Classifier\n",
    "\n",
    "The Naïve Bayes classifier combines the Naïve Bayes model with a _decision rule_, meaning a scheme that decides what label to assign to a given feature vector.\n",
    "\n",
    "One common rule is to pick the hypothesis that is most probable. This approach is known as the _maximum a posteriori_ or MAP decision rule. The corresponding _Bayes classifier_ assigns a class label ${\\displaystyle {\\hat {y}}=C_{k}}$ for some $k$ as follows:\n",
    "\n",
    "$${\\displaystyle {\\hat {y}}={\\underset {k\\in \\{1,\\dots ,K\\}}{\\operatorname {argmax} }}\\ p(C_{k})\\displaystyle \\prod _{i=1}^{n}p(f_{i}\\mid C_{k}).} \\tag{3}$$\n",
    "\n",
    "Unfortunately, an \"obvious\" implementation of this rule can have numerical instabilities because it requires multiplying exponentials with very different numerical ranges. Moreover, we can end up with very small numbers, which reduce accuracy and computation performance. To avoid these issues, can instead take argmax of _logarithm_ of the posterior, which is more stable and produces the same result. (Recall Problem 9 of the Practice Problems Midterm 1!) Applying $\\log$ to equation (3) yields\n",
    "\n",
    "$${\\displaystyle {\\hat {y}} = {\\underset {k\\in \\{1,\\dots ,K\\}}{\\operatorname {argmax} }}\\log \\left( p(C_{k})\\displaystyle \\prod _{i=1}^{n}p(f_{i}\\mid C_{k}) \\right) = {\\underset {k\\in \\{1,\\dots ,K\\}}{\\operatorname {argmax} }}\\ \\log p(C_{k}) +\\displaystyle \\sum _{i=1}^{n}\\log p(f_{i}\\mid C_{k})} \\tag{4}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You already wrote code to compute $p(C_k)$, so now let's work on the second term of (4), the log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Computing log-likelihood of a single class.** The rightmost sum of equation (4) above is **log-likelihood.** In particular, it is the logarithm of likelihood of class $k$ given the feature vector $\\mathbf{f}$.\n",
    "\n",
    "To discern its mathematical form, suppose we substitute equation (2) into the log-likelihood term. Then,\n",
    "\n",
    "$$ L(\\mathbf {f} \\mid C_{k}) =  \\sum _{i=1}^{n}\\log p(f_{i}\\mid C_{k}) = \\sum _{i=1}^{n} \\log \\left( {\\frac {1}{\\sqrt {2\\pi \\sigma _{i,k}^{2}}}}\\,\\exp\\left(-{\\frac {(f_i-\\mu _{i,k})^{2}}{2\\sigma _{i,k}^{2}}}\\right) \\right) = \n",
    "\\sum _{i=1}^{n} \\left( -0.5 \\log (2 \\pi \\sigma _{i,k}^{2}) - 0.5 \\frac {(f_i-\\mu _{i,k})^{2}}{\\sigma _{i,k}^{2}} \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2.a** (1 point). Complete the `log_likelihood(x, m, v)` function, below. The inputs are:\n",
    "\n",
    "- `x`, a feature vector $\\mathbf{f}$ of **one** data point from, say, `x_test`, which you'll recall is a dictionary with features as keys and scores as values;\n",
    "- `m`, a dictionary of means $\\mu _{i,k}$ for a single class $k$, with features as keys and their mean scores as values.\n",
    "- `v` is a dictionary of variances $\\sigma _{i,k}^{2}$ for a single class $k$, with features as keys and the variance of their scores as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(x, m, v):\n",
    "    return -0.5 *  sum([math.log(2 * math.pi * v[f]) + ((x[f] - m[f])**2) / v[f] for f in x.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_logprob",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test cell: `test_logprob`\n",
    "\n",
    "#Test Case 1\n",
    "training_data = {'feature_1': 0.58, 'feature_2': 0.55, 'feature_3': 0.57, 'feature_4': 0.7, 'feature_5': 0.74}\n",
    "mean1_test = {'feature_1': 0.435, 'feature_2': 0.415, 'feature_3': 0.535, 'feature_4': 0.56, 'feature_5': 0.62}\n",
    "var1_test = {'feature_1': 0.021025, 'feature_2': 0.018225, 'feature_3': 0.001225, 'feature_4': 0.0196, 'feature_5': 0.0144}\n",
    "\n",
    "res1 = log_likelihood(training_data,mean1_test,var1_test)\n",
    "assert math.isclose(res1, 4.277592981147556, abs_tol=1e-8), f\"The Output for Test Case 1 is incorrect, returned: \\n{res1:.8f}\\n, should be: \\n{4.277592981147556}\\n\"\n",
    "\n",
    "#Test Case 2\n",
    "training_data2 = {'feature_1': 0.9238, 'feature_2': 0.34, 'feature_3': 0.57, 'feature_4': 0.7, 'feature_5': 0.747}\n",
    "mean2_test = {'feature_1': 0.56835, 'feature_2': 0.1814, 'feature_3': 0.4975, 'feature_4': 0.7675, 'feature_5': 0.667}\n",
    "var2_test = {'feature_1': 0.1263447, 'feature_2': 0.02515396, 'feature_3': 0.00525625, 'feature_4': 0.00455625, 'feature_5': 0.0064}\n",
    "\n",
    "res2 = log_likelihood(training_data2,mean2_test,var2_test)\n",
    "assert math.isclose(res2, 3.6265730328980883, abs_tol=1e-8), f\"The Output for Test Case 2 is incorrect, returned: \\n{res2:.8f}\\n, should be: \\n{3.6265730328980883}\\n\"\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Implementing Gaussian Naive Bayes classifier.** You now have everything you need to implement the Gaussian naïve Bayes classifier from equation (4).\n",
    "\n",
    "$${\\displaystyle {\\hat {y}} = {\\underset {k\\in \\{1,\\dots ,K\\}}{\\operatorname {argmax} }}\\ \\log p(C_{k}) +\\displaystyle \\sum _{i=1}^{n}\\log p(f_{i}\\mid C_{k})} \\tag{4}$$\n",
    "\n",
    "In particular, recall that you have written these functions:\n",
    "- `prior()`, which returns **prior** for each class $C_k$;\n",
    "- `likelihood`, which returns **mean** and **variance** parameters for likelihood distributed as Gaussian;\n",
    "- and `log_likelihood`, which returns the logarithm of likelihood for Gaussian with given **mean** and **variance**.\n",
    "\n",
    "Let's now implement a function, `naive_bayes_classifier`, so that it returns a class prediction for testing features $\\mathbf{x}$ given **mean** and **variance** parameters for the likelihood and **prior** vector for classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2.b** (4 points: 2 points \"exposed\" and 2 points hidden).\n",
    "\n",
    "Complete `naive_bayes_classifier(x, dist_mean, dist_var, prior)` function, using the `log_likelihood()` function and equation (4). Here the inputs are\n",
    "\n",
    "- `x`, which is a full list of test samples (e.g., `x == x_test`);\n",
    "- `dist_mean` and `dist_var`, which are the results of a call to `likelihood()`;\n",
    "- and `prior()`, which is a dict of class priors as returned by a call to `prior()`.\n",
    "\n",
    "Your function should return a _list of strings_. Each element in the list would be the _predicted class label_ for the i-th data point in the `x_test` test sample. For additional reference, the output of the function should be similar to the contents of the _list_ of class labels in `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(x_test, dist_mean, dist_var, prior):\n",
    "    y_pred = []\n",
    "    for x in x_test:\n",
    "        pred = {l : math.log(prior[l]) + log_likelihood(x, dist_mean[l], dist_var[l]) for l in dist_mean.keys()}\n",
    "        y_pred.append(max(pred.items(), key=lambda x:x[1])[0])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_nbclassifier_1",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test cell: `test_nbclassifier_1`\n",
    "\n",
    "#Test Case 1\n",
    "nb_test_data_1 = [{'feature_1': 0.58, 'feature_2': 0.55, 'feature_3': 0.57, 'feature_4': 0.7, 'feature_5': 0.74},\n",
    "                  {'feature_1': 0.38, 'feature_2': 0.44, 'feature_3': 0.43, 'feature_4': 0.2, 'feature_5': 0.31},\n",
    "                  {'feature_1': 0.29, 'feature_2': 0.28, 'feature_3': 0.5, 'feature_4': 0.42, 'feature_5': 0.5},\n",
    "                  {'feature_1': 0.98, 'feature_2': 0.74, 'feature_3': 0.32, 'feature_4': 0.25, 'feature_5': 0.11},\n",
    "                  {'feature_1': 0.08, 'feature_2': 0.69, 'feature_3': 0.84, 'feature_4': 0.85, 'feature_5': 0.17} ] \n",
    "\n",
    "nb_p1 = {'class_1': 0.4, 'class_2': 0.6}\n",
    "nb_m1 = {'class_1': {'feature_1': 0.43499999999999994, 'feature_2': 0.41500000000000004, 'feature_3': 0.5349999999999999, 'feature_4': 0.5599999999999999, 'feature_5': 0.62}, 'class_2': {'feature_1': 0.48, 'feature_2': 0.6233333333333333, 'feature_3': 0.5299999999999999, 'feature_4': 0.43333333333333335, 'feature_5': 0.19666666666666666}}\n",
    "nb_v1 = {'class_1': {'feature_1': 0.021024999999999995, 'feature_2': 0.018225, 'feature_3': 0.0012249999999999982, 'feature_4': 0.019599999999999996, 'feature_5': 0.0144}, 'class_2': {'feature_1': 0.13999999999999999, 'feature_2': 0.01722222222222222, 'feature_3': 0.050066666666666655, 'feature_4': 0.08722222222222221, 'feature_5': 0.007022222222222222}}\n",
    "\n",
    "pred1 = naive_bayes_classifier(nb_test_data_1, nb_m1, nb_v1, nb_p1)\n",
    "assert pred1, f\"The resultant list for Test Case 1 is empty.\"\n",
    "pred_test1 = [\"class_1\",\"class_2\",\"class_1\",\"class_2\",\"class_2\"]\n",
    "\n",
    "for n, x in enumerate (pred1):\n",
    "    assert x == pred_test1[n], \"The result for Test Case 1 is incorrect\" \n",
    "    \n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_nbclassifier_2",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This test cell will be replaced with one hidden test case.\n",
      "You will only know the result after submitting to the autograder.\n",
      "If the autograder times out, then either your solution is highly\n",
      "inefficient or contains a bug (e.g., an infinite loop).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test cell: `test_nbclassifier_2`\n",
    "\n",
    "print(\"\"\"\n",
    "This test cell will be replaced with one hidden test case.\n",
    "You will only know the result after submitting to the autograder.\n",
    "If the autograder times out, then either your solution is highly\n",
    "inefficient or contains a bug (e.g., an infinite loop).\n",
    "\"\"\")\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Summary (no additional exercises beyond this point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If you've completed the above, you have implemented a Gaussian naïve Bayes classifier! The remaining cells run your classifier and assess its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Output:\n",
      "\n",
      "['class_3', 'class_5', 'class_2', 'class_4', 'class_1', 'class_1', 'class_2', 'class_2', 'class_1', 'class_5', 'class_1', 'class_1', 'class_1', 'class_3', 'class_1', 'class_3', 'class_1', 'class_1', 'class_1', 'class_4', 'class_4', 'class_3', 'class_1', 'class_3', 'class_3', 'class_2', 'class_1', 'class_3', 'class_1', 'class_1', 'class_2', 'class_2', 'class_2', 'class_1', 'class_1', 'class_4', 'class_1', 'class_1', 'class_5', 'class_2', 'class_2', 'class_1', 'class_1', 'class_1', 'class_1', 'class_3', 'class_2', 'class_1', 'class_5', 'class_3', 'class_4', 'class_4', 'class_2', 'class_1', 'class_3', 'class_1', 'class_4', 'class_3', 'class_4', 'class_4', 'class_2', 'class_3', 'class_2', 'class_5', 'class_4', 'class_4', 'class_4', 'class_5', 'class_5', 'class_1', 'class_1', 'class_2', 'class_3', 'class_3', 'class_4', 'class_4', 'class_1', 'class_5', 'class_4', 'class_3', 'class_3', 'class_2', 'class_1', 'class_1', 'class_2', 'class_3', 'class_3', 'class_1', 'class_2', 'class_4', 'class_2', 'class_1', 'class_1', 'class_3', 'class_1', 'class_1', 'class_3', 'class_2', 'class_1', 'class_2', 'class_1', 'class_3', 'class_2', 'class_1', 'class_1', 'class_4', 'class_4', 'class_1', 'class_1']\n",
      "\n",
      "Pretty Printing to display only the class number: \n",
      "\n",
      "[3, 5, 2, 4, 1, 1, 2, 2, 1, 5, 1, 1, 1, 3, 1, 3, 1, 1, 1, 4, 4, 3, 1, 3, 3, 2, 1, 3, 1, 1, 2, 2, 2, 1, 1, 4, 1, 1, 5, 2, 2, 1, 1, 1, 1, 3, 2, 1, 5, 3, 4, 4, 2, 1, 3, 1, 4, 3, 4, 4, 2, 3, 2, 5, 4, 4, 4, 5, 5, 1, 1, 2, 3, 3, 4, 4, 1, 5, 4, 3, 3, 2, 1, 1, 2, 3, 3, 1, 2, 4, 2, 1, 1, 3, 1, 1, 3, 2, 1, 2, 1, 3, 2, 1, 1, 4, 4, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Classifier Output:\\n\")\n",
    "nb_result = naive_bayes_classifier(x_test, dist_mean, dist_var, prior_val)\n",
    "print(nb_result)\n",
    "\n",
    "print(\"\\nPretty Printing to display only the class number: \\n\")\n",
    "pred = [int(s[-1]) for s in nb_result]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "And finally, below you can see accuracy of our classifier, as well as its per-class statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ecoli-mod.mat...\n",
      "Accuracy: 0.8348623853211009\n",
      "\n",
      "\n",
      "Class 1\n",
      "Prediction positive: 41\n",
      "Condition positive: 40\n",
      "True positive: 39\n",
      "Precision: 0.9512195121951219\n",
      "Recall: 0.975\n",
      "\n",
      "\n",
      "Class 2\n",
      "Prediction positive: 21\n",
      "Condition positive: 26\n",
      "True positive: 17\n",
      "Precision: 0.8095238095238095\n",
      "Recall: 0.6538461538461539\n",
      "\n",
      "\n",
      "Class 3\n",
      "Prediction positive: 21\n",
      "Condition positive: 21\n",
      "True positive: 19\n",
      "Precision: 0.9047619047619048\n",
      "Recall: 0.9047619047619048\n",
      "\n",
      "\n",
      "Class 4\n",
      "Prediction positive: 18\n",
      "Condition positive: 13\n",
      "True positive: 9\n",
      "Precision: 0.5\n",
      "Recall: 0.6923076923076923\n",
      "\n",
      "\n",
      "Class 5\n",
      "Prediction positive: 8\n",
      "Condition positive: 9\n",
      "True positive: 7\n",
      "Precision: 0.875\n",
      "Recall: 0.7777777777777778\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from problem_utils import assess_accuracy\n",
    "assess_accuracy('ecoli-mod.mat', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Fin!** Remember to test your solutions by running them as the autograder will: restart the kernel and run all cells from \"top-to-bottom.\" Also remember to submit to the autograder; otherwise, you will not get credit for your hard work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
