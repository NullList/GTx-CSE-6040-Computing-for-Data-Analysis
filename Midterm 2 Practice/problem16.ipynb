{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 16: Data cleaning with Pandas\n",
    "\n",
    "_Version 1.8_\n",
    "\n",
    "**Pro-tips.** If your program behavior seem strange, try resetting the kernel and rerunning everything. If you mess up this notebook or just want to start from scratch, save copies of all your partial responses and use `Actions` $\\rightarrow$ `Reset Assignment` to get a fresh, original copy of this notebook. (_Resetting will wipe out any answers you've written so far, so be sure to stash those somewhere safe if you intend to keep or reuse them!_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Often (nearly always) the data we are working with will need some type of cleaning or rearranging before it can be used. In this problem, you will be working with some messy data. The dataset that we will be working with is called \"sales.csv\". It contains some ticket sale data, accessed from [DataCamp's R Tutorial](https://www.datacamp.com/courses/importing-cleaning-data-in-r-case-studies). You will be completing this problem with Pandas. \n",
    "\n",
    "Our goal will be to take this messy data set, and after doing some data cleaning, we will determine which 5 cities have the highest ticket prices on average, after accounting for inflation. The excercises will step you through this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 0** (ungraded). Before we can begin, let's import the dataset, called `sales.csv`, into a pandas dataframe. We will alias it as `data`. Use `data.head()` to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>event_id</th>\n",
       "      <th>primary_act_id</th>\n",
       "      <th>secondary_act_id</th>\n",
       "      <th>purch_party_lkup_id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>primary_act_name</th>\n",
       "      <th>secondary_act_name</th>\n",
       "      <th>major_cat_name</th>\n",
       "      <th>minor_cat_name</th>\n",
       "      <th>...</th>\n",
       "      <th>edu_1st_indv_val</th>\n",
       "      <th>edu_2nd_indv_val</th>\n",
       "      <th>adults_in_hh_num</th>\n",
       "      <th>married_ind</th>\n",
       "      <th>child_present_ind</th>\n",
       "      <th>home_owner_ind</th>\n",
       "      <th>occpn_val</th>\n",
       "      <th>occpn_1st_val</th>\n",
       "      <th>occpn_2nd_val</th>\n",
       "      <th>dist_to_ven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>abcaf1adb99a935fc661</td>\n",
       "      <td>43f0436b905bfa7c2eec</td>\n",
       "      <td>b85143bf51323b72e53c</td>\n",
       "      <td>7dfa56dd7d5956b17587</td>\n",
       "      <td>Xfinity Center Mansfield Premier Parking: Flor...</td>\n",
       "      <td>XFINITY Center Mansfield Premier Parking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MISC</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6c56d7f08c95f2aa453c</td>\n",
       "      <td>1a3e9aecd0617706a794</td>\n",
       "      <td>f53529c5679ea6ca5a48</td>\n",
       "      <td>4f9e6fc637eaf7b736c2</td>\n",
       "      <td>Gorge Camping - dave matthews band - sept 3-7</td>\n",
       "      <td>Gorge Camping</td>\n",
       "      <td>Dave Matthews Band</td>\n",
       "      <td>MISC</td>\n",
       "      <td>CAMPING</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c7ab4524a121f9d687d2</td>\n",
       "      <td>4b677c3f5bec71eec8d1</td>\n",
       "      <td>b85143bf51323b72e53c</td>\n",
       "      <td>6c2545703bd527a7144d</td>\n",
       "      <td>Dodge Theatre Adams Street Parking - benise</td>\n",
       "      <td>Parking Event</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MISC</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>394cb493f893be9b9ed1</td>\n",
       "      <td>b1ccea01ad6ef8522796</td>\n",
       "      <td>b85143bf51323b72e53c</td>\n",
       "      <td>527d6b1eaffc69ddd882</td>\n",
       "      <td>Gexa Energy Pavilion Vip Parking : kid rock wi...</td>\n",
       "      <td>Gexa Energy Pavilion VIP Parking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MISC</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>55b5f67e618557929f48</td>\n",
       "      <td>91c03a34b562436efa3c</td>\n",
       "      <td>b85143bf51323b72e53c</td>\n",
       "      <td>8bd62c394a35213bdf52</td>\n",
       "      <td>Premier Parking - motley crue</td>\n",
       "      <td>White River Amphitheatre Premier Parking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MISC</td>\n",
       "      <td>PARKING</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              event_id        primary_act_id  \\\n",
       "0           1  abcaf1adb99a935fc661  43f0436b905bfa7c2eec   \n",
       "1           2  6c56d7f08c95f2aa453c  1a3e9aecd0617706a794   \n",
       "2           3  c7ab4524a121f9d687d2  4b677c3f5bec71eec8d1   \n",
       "3           4  394cb493f893be9b9ed1  b1ccea01ad6ef8522796   \n",
       "4           5  55b5f67e618557929f48  91c03a34b562436efa3c   \n",
       "\n",
       "       secondary_act_id   purch_party_lkup_id  \\\n",
       "0  b85143bf51323b72e53c  7dfa56dd7d5956b17587   \n",
       "1  f53529c5679ea6ca5a48  4f9e6fc637eaf7b736c2   \n",
       "2  b85143bf51323b72e53c  6c2545703bd527a7144d   \n",
       "3  b85143bf51323b72e53c  527d6b1eaffc69ddd882   \n",
       "4  b85143bf51323b72e53c  8bd62c394a35213bdf52   \n",
       "\n",
       "                                          event_name  \\\n",
       "0  Xfinity Center Mansfield Premier Parking: Flor...   \n",
       "1      Gorge Camping - dave matthews band - sept 3-7   \n",
       "2        Dodge Theatre Adams Street Parking - benise   \n",
       "3  Gexa Energy Pavilion Vip Parking : kid rock wi...   \n",
       "4                      Premier Parking - motley crue   \n",
       "\n",
       "                           primary_act_name  secondary_act_name  \\\n",
       "0  XFINITY Center Mansfield Premier Parking                 NaN   \n",
       "1                             Gorge Camping  Dave Matthews Band   \n",
       "2                             Parking Event                 NaN   \n",
       "3          Gexa Energy Pavilion VIP Parking                 NaN   \n",
       "4  White River Amphitheatre Premier Parking                 NaN   \n",
       "\n",
       "  major_cat_name minor_cat_name  ... edu_1st_indv_val edu_2nd_indv_val  \\\n",
       "0           MISC        PARKING  ...              NaN              NaN   \n",
       "1           MISC        CAMPING  ...              NaN              NaN   \n",
       "2           MISC        PARKING  ...              NaN              NaN   \n",
       "3           MISC        PARKING  ...              NaN              NaN   \n",
       "4           MISC        PARKING  ...              NaN              NaN   \n",
       "\n",
       "  adults_in_hh_num  married_ind  child_present_ind home_owner_ind occpn_val  \\\n",
       "0              NaN          NaN                NaN            NaN       NaN   \n",
       "1              NaN          NaN                NaN            NaN       NaN   \n",
       "2              NaN          NaN                NaN            NaN       NaN   \n",
       "3              NaN          NaN                NaN            NaN       NaN   \n",
       "4              NaN          NaN                NaN            NaN       NaN   \n",
       "\n",
       "  occpn_1st_val occpn_2nd_val dist_to_ven  \n",
       "0           NaN           NaN         NaN  \n",
       "1           NaN           NaN        59.0  \n",
       "2           NaN           NaN         NaN  \n",
       "3           NaN           NaN         NaN  \n",
       "4           NaN           NaN         NaN  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"resource/asnlib/publicdata/sales.csv\", encoding='latin1')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert data.shape[0] == (5000),\"Number of rows is incorrect\"\n",
    "assert data.shape[1] == (46),\"Number of collumns is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1** (2 points). We have some null values and a lot of unnecessary columns in our `data`. We should drop the rows that have a null value in `onsale_dt` and only keep the columns that we need for this task: `event_disp_name, tickets_purchased_qty, trans_face_val_amt, onsale_dt, venue_city, and venue_state.` \n",
    "\n",
    "Let's alias this dataframe as `data_clean`. Please makes sure your indices are preserved. Do not change the name of your data frame going forward or the autograder won't work. \n",
    "\n",
    "Using `inplace=True` will be useful for you to do this. You need to be careful when using this, however. Because it modifies the current dataframe you can run into errors that you may not expect. For example, if you drop a column, that column will no longer exist in your dataframe. Trying to rerun the cell will cause an error because you will be trying to drop a column that doesn't exist. \n",
    "\n",
    "Just make sure your final data frame in each section is called `data_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dropna(subset = ['onsale_dt'], inplace=True)\n",
    "columns = data.columns.values\n",
    "want = ['event_disp_name', 'tickets_purchased_qty', 'trans_face_val_amt', 'onsale_dt', 'venue_city', 'venue_state']\n",
    "get_rid = []\n",
    "for column in columns:\n",
    "    if column not in want:\n",
    "        get_rid += [column]\n",
    "get_rid\n",
    "data_clean = data.drop(columns=get_rid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise1",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test: Exercise1\n",
    "\n",
    "assert data_clean.shape == (4899,6), \"Your dataframe is not the right shape\"\n",
    "assert data_clean.shape[0] == (4899), \"Number of rows is wrong\"\n",
    "assert data_clean.shape[1] == (6), \"Number of columns is wrong\"\n",
    "assert data_clean.index.max() == 4999, \"Your index is wrong\"\n",
    "assert data_clean.index.min() == 0, \"Your index is wrong\"\n",
    "print(\"\\n(Passed!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2** (4 points total -- 1 point \"exposed\" and 3 points hidden).\n",
    "\n",
    "Now that we have the data down to the 6 columns we care about, we can continue.\n",
    "\n",
    "You'll notice that we do not yet have a per ticket price. So that we are comparing apples to apples, we must  divide the face value amount by the quantity of tickets purchased, then increase this by the appropriate amount of inflation based on the year the tickets were sold to the general public (`onsale_dt`).  Alias this as `per_ticket_price`. Make sure to round it to two decimal places using the built in round function (round()), since we are working with dollar values. Only round at the end.\n",
    "\n",
    "We are provided the [inflation rates](https://www.thebalance.com/u-s-inflation-rate-history-by-year-and-forecast-3306093) below for each respective year in the dataset. Increase the `per_ticket_price` by this amount, depending on the year the tickets were sold to the general public (`onsale_dt`). You will need to pull the year from `onsale_dt` to do this. When you do, alias it as 'year' and store the values as strings. Your dataframe should now have two new columns that are labeled `'per_ticket_price'` and `'year'`. There is no need to multiply inflation rates together. Just take each year's inflation rate as independent in your calculations. \n",
    "\n",
    "Additionally, now please reset your index after doing the calculations.\n",
    "\n",
    "| 2004 | 2005 | 2006 | 2007 | 2008 | 2009 | 2010 | 2011 | 2012 | 2013 | 2014 | 2015 | 2016 |\n",
    "|------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
    "| 3.3% | 3.4% | 2.5% | 4.1% | 0.1% | 2.7% | 1.5% | 3.0% | 1.7% | 1.5% | 0.8% | 0.7% | 2.1% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_clean['year'] = data_clean['onsale_dt'].map(lambda x: x[0:4])\n",
    "years = np.arange(2004, 2017)\n",
    "years = [str(year) for year in years]\n",
    "inflation = np.array([0.033, 0.034, 0.025, 0.041, 0.001, 0.027, 0.015, 0.030, 0.017, 0.015, 0.008, 0.007, 0.021]) \n",
    "\n",
    "inflation_table = {}\n",
    "for i in range(len(years)):\n",
    "    inflation_table[years[i]] = 1 + inflation[i]\n",
    "data_clean['per_ticket_price'] = data_clean.apply(lambda x: round(float(inflation_table[str(x['year'])]) * float(x['trans_face_val_amt']) / float(x['tickets_purchased_qty']), 2), axis=1)\n",
    "data_clean = data_clean.reset_index()\n",
    "data_clean.drop(columns = ['index'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise2",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "#Test Ex 2\n",
    "assert(len(data_clean['year']) == 4899), 'Your dataset is the wrong length'\n",
    "index = np.random.choice(len(data_clean['year']))\n",
    "assert type(data_clean['year'][index]) == str, 'Years are wrong type'\n",
    "assert data_clean.shape == (4899,8), \"Your dataframe is not the correct shape\"\n",
    "assert abs(data_clean['per_ticket_price'][0] - 45.31) < 0.02\n",
    "assert abs(data_clean['per_ticket_price'][1] - 77.02) < 0.02\n",
    "assert abs(data_clean['per_ticket_price'][2] - 5.12) < 0.02\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise2_hidden",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test: Exercise2\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 3** (1 point). The fields `venue_city` and `venue_state` are unfortunately not in the format that we need. Because we will later be asking for the cities with the highest ticket prices, we should combine `venue_city` and `venue_state` in a City, State format (ex. `Atlanta, Georgia`). Doing so will avoid confusion and errors related to cities that are in more than one state ([Springfield](https://www.worldatlas.com/articles/most-common-town-and-city-names-in-the-u-s-a.html) or [Gainesville](https://en.wikipedia.org/wiki/Gainesville) for example). Save this new column as `location`, and drop `venue_city` and `venue_state`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_clean['location'] = data_clean['venue_city'] + ', ' + data_clean['venue_state']\n",
    "data_clean.drop(columns = ['venue_city', 'venue_state'], inplace = True)\n",
    "\n",
    "data_clean = data_clean.drop(columns = ['event_disp_name', 'tickets_purchased_qty', 'trans_face_val_amt', 'onsale_dt', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise3",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test: Exercise3 \n",
    "\n",
    "assert 'location' in data_clean.columns, \"Did you add location?\"\n",
    "assert 'venue_city' not in data_clean.columns, \"Did you drop venue_city?\"\n",
    "assert 'venue_state' not in data_clean.columns, \"Did you drop venue_state?\"\n",
    "assert data_clean['location'][0] == 'MANSFIELD, MASSACHUSETTS'\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 4** (2 points). \n",
    "Finally, let's find the average ticket price per city. List only the `location` and `per_ticket_price` for the **top 5 cities.** Call this dataframe `highest_cities` and please make sure its index is 0, 1, 2, 3, 4 (*hint the index can be accomplished with just one function call*). Remember to round your price to two decimal places using round().\n",
    "\n",
    "In the cell below you can find both a sample solution and a sample dataset for you to try out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>per_ticket_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOS ANGELES, CALIFORNIA</td>\n",
       "      <td>15.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATLANTA, GEORGIA</td>\n",
       "      <td>14.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  location  per_ticket_price\n",
       "0  LOS ANGELES, CALIFORNIA             15.16\n",
       "1         ATLANTA, GEORGIA             14.11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = pd.DataFrame()\n",
    "sample_data['per_ticket_price'] = [4.80, 15.16, 23.42]\n",
    "sample_data['location'] = ['ATLANTA, GEORGIA', 'LOS ANGELES, CALIFORNIA', 'ATLANTA, GEORGIA']\n",
    "\n",
    "sample_sol = pd.DataFrame()\n",
    "sample_sol['location'] = ['LOS ANGELES, CALIFORNIA', 'ATLANTA, GEORGIA']\n",
    "sample_sol['per_ticket_price'] = [15.16, 14.11]\n",
    "\n",
    "sample_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highest_cities = data_clean.groupby('location').agg(np.mean).sort_values('per_ticket_price', ascending=False).apply(lambda x: round(x, 2))[0:5]\n",
    "highest_cities = highest_cities.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise4",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test: Exercise4\n",
    "assert(highest_cities.iloc[0][0] == 'READING, PENNSYLVANIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Exercise4_hidden",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test: Exercise4_hidden\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Fin!** You’ve reached the end of this part. Don’t forget to restart and run all cells again to make sure it’s all working when run in sequence; and make sure your work passes the submission process. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
